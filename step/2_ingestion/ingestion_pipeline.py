"""
INGESTION PIPELINE - GIAI ÄOáº N 2
ÄÆ°a dá»¯ liá»‡u luáº­t phÃ¡p vÃ o Vector Database (FAISS)

Author: AI Engineer
Date: 2026-01-31
"""

import json
import os
import pickle
from typing import List, Dict
from pathlib import Path

from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS


# ============================================================================
# CONFIGURATION
# ============================================================================

# ÄÆ°á»ng dáº«n input/output
INPUT_DIR = Path("../../data/input")
OUTPUT_DIR = Path("./output")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# CÃ¡c file JSON cáº§n xá»­ lÃ½
INPUT_FILES = [
    "luatdedieu.json",
    "luatkhituongthuyvan.json",
    "luatphongchongthientai.json",
    "luatthuyloi.json"
]

# Embedding model configuration
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
EMBEDDING_DEVICE = "cpu"  # Äá»•i thÃ nh "cuda" náº¿u cÃ³ GPU

# FAISS index configuration
FAISS_INDEX_NAME = "law_documents_index"


# ============================================================================
# STEP 1: Äá»ŒC Dá»® LIá»†U Tá»ª JSON FILES
# ============================================================================

def load_json_data(file_path: Path) -> List[Dict]:
    """
    Äá»c dá»¯ liá»‡u tá»« file JSON
    
    Args:
        file_path: ÄÆ°á»ng dáº«n tá»›i file JSON
        
    Returns:
        List cÃ¡c dictionary chá»©a dá»¯ liá»‡u vÄƒn báº£n luáº­t
    """
    print(f"ğŸ“– Äang Ä‘á»c file: {file_path.name}")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"   âœ“ ÄÃ£ Ä‘á»c {len(data)} records")
    return data


# ============================================================================
# STEP 2: CHUYá»‚N Äá»”I THÃ€NH LANGCHAIN DOCUMENTS
# ============================================================================

def create_documents(json_data: List[Dict]) -> List[Document]:
    """
    Chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u JSON thÃ nh LangChain Document objects
    
    Má»—i Document cÃ³:
    - page_content: Ná»™i dung vÄƒn báº£n luáº­t (tá»« field content_for_embedding)
    - metadata: Giá»¯ nguyÃªn toÃ n bá»™ metadata tá»« JSON (id, doc_id, doc_name, 
                chapter_no, chapter_name, article_no, article_name, type, citation)
    
    Args:
        json_data: List cÃ¡c dictionary tá»« JSON
        
    Returns:
        List cÃ¡c LangChain Document objects
    """
    print(f"\nğŸ”„ Äang chuyá»ƒn Ä‘á»•i {len(json_data)} records thÃ nh Documents...")
    
    documents = []
    
    for idx, record in enumerate(json_data):
        # Táº¡o metadata - giá»¯ nguyÃªn nhÆ° trong JSON, khÃ´ng thÃªm field má»›i
        metadata = {
            "id": record["id"],
            "doc_id": record["metadata"]["doc_id"],
            "doc_name": record["metadata"]["doc_name"],
            "chapter_no": record["metadata"]["chapter_no"],
            "chapter_name": record["metadata"]["chapter_name"],
            "article_no": record["metadata"]["article_no"],
            "article_name": record["metadata"]["article_name"],
            "type": record["metadata"]["type"],
            "citation": record["citation"]
        }
        
        # Táº¡o Document object
        doc = Document(
            page_content=record["content_for_embedding"],
            metadata=metadata
        )
        
        documents.append(doc)
        
        # Progress indicator
        if (idx + 1) % 50 == 0:
            print(f"   ÄÃ£ xá»­ lÃ½ {idx + 1}/{len(json_data)} documents...")
    
    print(f"   âœ“ HoÃ n thÃ nh! Tá»•ng cá»™ng {len(documents)} Documents")
    return documents


# ============================================================================
# STEP 3: Táº O EMBEDDINGS VÃ€ LÆ¯U VÃ€O FAISS
# ============================================================================

def create_vector_store(documents: List[Document]) -> FAISS:
    """
    Táº¡o embeddings cho documents vÃ  lÆ°u vÃ o FAISS vector store
    
    Args:
        documents: List cÃ¡c LangChain Document objects
        
    Returns:
        FAISS vector store Ä‘Ã£ Ä‘Æ°á»£c táº¡o
    """
    print(f"\nğŸ§  Äang khá»Ÿi táº¡o Embedding Model: {EMBEDDING_MODEL_NAME}")
    print(f"   (Model sáº½ Ä‘Æ°á»£c táº£i vá» láº§n Ä‘áº§u tiÃªn, cÃ³ thá»ƒ máº¥t vÃ i phÃºt...)")
    
    # Khá»Ÿi táº¡o embedding model
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs={'device': EMBEDDING_DEVICE},
        encode_kwargs={'normalize_embeddings': True}  # Chuáº©n hÃ³a Ä‘á»ƒ tÃ­nh cosine similarity
    )
    
    print(f"   âœ“ Model Ä‘Ã£ sáºµn sÃ ng!")
    
    print(f"\nğŸ”¢ Äang táº¡o embeddings cho {len(documents)} documents...")
    print(f"   (QuÃ¡ trÃ¬nh nÃ y cÃ³ thá»ƒ máº¥t vÃ i phÃºt...)")
    
    # Táº¡o FAISS vector store tá»« documents
    # FAISS sáº½ tá»± Ä‘á»™ng:
    # 1. Táº¡o embeddings cho táº¥t cáº£ documents
    # 2. XÃ¢y dá»±ng index Ä‘á»ƒ tÃ¬m kiáº¿m nhanh
    # 3. LÆ°u trá»¯ metadata kÃ¨m theo
    vectorstore = FAISS.from_documents(
        documents=documents,
        embedding=embeddings
    )
    
    print(f"   âœ“ HoÃ n thÃ nh! Vector store Ä‘Ã£ Ä‘Æ°á»£c táº¡o")
    print(f"   ğŸ“Š Sá»‘ lÆ°á»£ng vectors: {vectorstore.index.ntotal}")
    print(f"   ğŸ“ Vector dimension: {vectorstore.index.d}")
    
    return vectorstore


# ============================================================================
# STEP 4: LÆ¯U FAISS INDEX VÃ€ METADATA
# ============================================================================

def save_vector_store(vectorstore: FAISS, base_name: str):
    """
    LÆ°u FAISS vector store ra file Ä‘á»ƒ sá»­ dá»¥ng láº¡i
    
    Args:
        vectorstore: FAISS vector store cáº§n lÆ°u
        base_name: TÃªn cÆ¡ sá»Ÿ cho cÃ¡c file output
    """
    print(f"\nğŸ’¾ Äang lÆ°u FAISS index vÃ o {OUTPUT_DIR}")
    
    # ÄÆ°á»ng dáº«n lÆ°u index
    index_path = OUTPUT_DIR / base_name
    
    # LÆ°u FAISS index (bao gá»“m vectors vÃ  metadata)
    vectorstore.save_local(str(index_path))
    
    print(f"   âœ“ ÄÃ£ lÆ°u index táº¡i: {index_path}")
    print(f"   ğŸ“ Files Ä‘Æ°á»£c táº¡o:")
    print(f"      - index.faiss: FAISS vector index")
    print(f"      - index.pkl: Document metadata vÃ  docstore")
    
    # LÆ°u thÃ´ng tin cáº¥u hÃ¬nh Ä‘á»ƒ reference
    config = {
        "embedding_model": EMBEDDING_MODEL_NAME,
        "total_documents": vectorstore.index.ntotal,
        "vector_dimension": vectorstore.index.d,
        "input_files": INPUT_FILES,
        "created_at": "2026-01-31"
    }
    
    config_path = OUTPUT_DIR / f"{base_name}_config.json"
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    print(f"      - {base_name}_config.json: ThÃ´ng tin cáº¥u hÃ¬nh")
    

# ============================================================================
# STEP 5: TEST LOAD VÃ€ RETRIEVAL
# ============================================================================

def test_vector_store(base_name: str):
    """
    Test kháº£ nÄƒng load láº¡i vector store vÃ  thá»±c hiá»‡n retrieval
    
    Args:
        base_name: TÃªn cÆ¡ sá»Ÿ cá»§a index Ä‘Ã£ lÆ°u
    """
    print(f"\nğŸ§ª Test load vector store vÃ  retrieval...")
    
    # Khá»Ÿi táº¡o láº¡i embedding model
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs={'device': EMBEDDING_DEVICE},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    # Load FAISS index tá»« disk
    index_path = OUTPUT_DIR / base_name
    vectorstore = FAISS.load_local(
        str(index_path),
        embeddings,
        allow_dangerous_deserialization=True  # Cáº§n thiáº¿t Ä‘á»ƒ load pickle
    )
    
    print(f"   âœ“ ÄÃ£ load thÃ nh cÃ´ng vector store")
    print(f"   ğŸ“Š Sá»‘ vectors: {vectorstore.index.ntotal}")
    
    # Test query
    test_query = "Quy Ä‘á»‹nh vá» báº£o vá»‡ Ä‘Ãª Ä‘iá»u"
    print(f"\n   ğŸ” Test query: '{test_query}'")
    
    results = vectorstore.similarity_search(test_query, k=3)
    
    print(f"   ğŸ“‹ Top 3 káº¿t quáº£ tÃ¬m kiáº¿m:")
    for i, doc in enumerate(results, 1):
        print(f"\n   [{i}] {doc.metadata['citation']}")
        print(f"       {doc.page_content[:150]}...")
    
    print(f"\n   âœ… Test thÃ nh cÃ´ng!")


# ============================================================================
# MAIN PIPELINE
# ============================================================================

def main():
    """
    Main function - Cháº¡y toÃ n bá»™ ingestion pipeline
    """
    print("=" * 80)
    print("INGESTION PIPELINE - GIAI ÄOáº N 2")
    print("ÄÆ°a dá»¯ liá»‡u luáº­t phÃ¡p vÃ o Vector Database")
    print("=" * 80)
    
    # BÆ°á»›c 1: Äá»c táº¥t cáº£ JSON files
    all_data = []
    for filename in INPUT_FILES:
        file_path = INPUT_DIR / filename
        if file_path.exists():
            data = load_json_data(file_path)
            all_data.extend(data)
        else:
            print(f"   âš ï¸ Warning: File khÃ´ng tá»“n táº¡i: {file_path}")
    
    print(f"\nğŸ“Š Tá»•ng cá»™ng: {len(all_data)} records tá»« {len(INPUT_FILES)} files")
    
    # BÆ°á»›c 2: Chuyá»ƒn Ä‘á»•i thÃ nh Documents
    documents = create_documents(all_data)
    
    # BÆ°á»›c 3: Táº¡o embeddings vÃ  vector store
    vectorstore = create_vector_store(documents)
    
    # BÆ°á»›c 4: LÆ°u vector store
    save_vector_store(vectorstore, FAISS_INDEX_NAME)
    
    # BÆ°á»›c 5: Test load vÃ  retrieval
    test_vector_store(FAISS_INDEX_NAME)
    
    print("\n" + "=" * 80)
    print("âœ… HOÃ€N THÃ€NH INGESTION PIPELINE!")
    print("=" * 80)
    print(f"\nğŸ“ Output directory: {OUTPUT_DIR.absolute()}")
    print(f"ğŸ“¦ FAISS index: {FAISS_INDEX_NAME}")
    print(f"ğŸ“Š Total vectors: {len(documents)}")
    print(f"\nğŸ¯ Báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng index nÃ y cho Retrieval trong bÆ°á»›c tiáº¿p theo!")


if __name__ == "__main__":
    main()
